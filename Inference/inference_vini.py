# -*- coding: utf-8 -*-
"""natural_language_inference.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/viniciusrpb/cic0269_natural_language_processing/blob/main/lectures/natural_language_inference.ipynb
"""


import pandas as pd
from keras.models import Sequential
from keras.layers import Dense, Embedding, Activation, SimpleRNN, BatchNormalization, RNN, Flatten, Input, LSTM, \
    Bidirectional
from sklearn.feature_extraction.text import TfidfVectorizer
from tensorflow.keras.optimizers import SGD
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from keras.utils.np_utils import to_categorical
from sklearn.metrics import classification_report
import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import StratifiedKFold

from tensorflow.keras.callbacks import EarlyStopping
from keras.datasets import reuters
from sklearn.metrics import f1_score
from sklearn.model_selection import train_test_split
import gensim
import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf
import tensorflow_datasets as tfds
import pandas as pd
from tensorflow.keras.optimizers import SGD
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Dense, Activation, Embedding, SimpleRNN, Dropout
from keras.utils.np_utils import to_categorical

ds_train = tfds.load('snli', split='train[50%:]', shuffle_files=True)
ds_valid = tfds.load('snli', split='validation', shuffle_files=False)
ds_test = tfds.load('snli', split='test', shuffle_files=False)

df_train = tfds.as_dataframe(ds_train.take(10000))
df_valid = tfds.as_dataframe(ds_valid.take(2500))
df_test = tfds.as_dataframe(ds_test.take(2500))


def preprocessDataFrame(df):
    dic = {}
    dic['premise_hypothesis'] = []
    dic['label'] = []

    hypothesis = [x.decode('utf-8') for x in df['hypothesis'].values]
    premise = [x.decode('utf-8') for x in df['premise'].values]

    for idx, sentence in enumerate(premise):
        dic['premise_hypothesis'].append(premise[idx] + " " + hypothesis[idx])
        dic['label'].append(df['label'][idx])

    return pd.DataFrame.from_dict(dic)


df_train = preprocessDataFrame(df_train)
df_valid = preprocessDataFrame(df_valid)
df_test = preprocessDataFrame(df_test)

df_train['label'] = pd.Categorical(df_train['label'])
y_train_int = df_train['label'].cat.codes

df_valid['label'] = pd.Categorical(df_valid['label'])
y_valid_int = df_valid['label'].cat.codes

df_test['label'] = pd.Categorical(df_test['label'])
y_test_int = df_test['label'].cat.codes

y_train = to_categorical(y_train_int)
y_valid = to_categorical(y_valid_int)
y_test = to_categorical(y_test_int)

train_sentences = df_train['premise_hypothesis'].to_list()

vocabulary = {}

for i in range(0, len(train_sentences)):
    train_sentences[i] = train_sentences[i].lower()
    for word in train_sentences[i].split():
        if word not in vocabulary:
            vocabulary[word] = 1
        else:
            vocabulary[word] += 1

tokenizer = Tokenizer(num_words=len(vocabulary))
tokenizer.fit_on_texts(df_train['premise_hypothesis'])
word2index = tokenizer.word_index
vocab_size = len(word2index)

train_sequences = tokenizer.texts_to_sequences(df_train['premise_hypothesis'])
valid_sequences = tokenizer.texts_to_sequences(df_valid['premise_hypothesis'])
test_sequences = tokenizer.texts_to_sequences(df_test['premise_hypothesis'])

trunc_type = 'post'
padding_type = 'post'
max_length = 64
vocab_size = len(vocabulary)

train_padded = pad_sequences(train_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)
valid_padded = pad_sequences(valid_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)
test_padded = pad_sequences(test_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)

model = Sequential()
model.add(Embedding(input_dim=vocab_size, output_dim=64, input_length=max_length))
model.add(SimpleRNN(64, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(4, activation="softmax"))
model.summary()

sgd = SGD(learning_rate=0.01)
model.compile(loss="categorical_crossentropy", optimizer=sgd, metrics=['accuracy'])
history = model.fit(train_padded, y_train, validation_data=(valid_padded, y_valid), epochs=30)

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['training set', 'validation set'], loc='upper left')
plt.show()

y_prob = model.predict(test_padded)
y_pred = np.argmax(y_prob, axis=1)
print(classification_report(y_test_int, y_pred))

